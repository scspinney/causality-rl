{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "from typing import Callable, Mapping, NamedTuple, Tuple, Sequence\n",
    "from functools import partial\n",
    "\n",
    "import jax # Autograd package\n",
    "import jax.numpy as jnp # GPU NumPy :)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque # Cyclic list with max capacity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The class which defines the graph, transitions, and solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below this cell is an experimentation section. Go to the last cell to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of adjacency matrix for 3 variables: x1, x2, x3\n",
    "\n",
    "\n",
    "\n",
    "adj_matrices = {\n",
    "    \n",
    "    # A1 describes a graph where the edges are x1 -> x2 -> x3 \n",
    "    \"A1\" : [[0,0,0],\n",
    "            [1,0,0],\n",
    "            [0,1,0]],\n",
    "\n",
    "    # A2 describes a graph where the edges are x2 -> x1 -> x3 \n",
    "    \"A2\" : [[0,1,0],\n",
    "            [0,0,0],\n",
    "            [1,0,0]],\n",
    "\n",
    "    # A3 : x3,x2 -> x1\n",
    "    \"A3\" : [[0,1,1],\n",
    "            [0,0,0],\n",
    "            [0,0,0]],\n",
    "\n",
    "    # A4 : x2, x1, x3 (independent)\n",
    "    \"A4\": [[0,0,0],\n",
    "            [0,0,0],\n",
    "            [0,0,0]],\n",
    "\n",
    "\n",
    "    # A5 : x1 -> x2, x1,x2 -> x3\n",
    "    \"A5\" : [[0,0,0],\n",
    "            [1,0,0],\n",
    "            [1,1,0]],\n",
    "\n",
    "    # A6 : x2,x1 -> x3\n",
    "    \"A6\" : [[0,0,0],\n",
    "            [0,0,0],\n",
    "            [1,1,0]]\n",
    "\n",
    "    # AC describes a fully connected, undirected graph (not causal)\n",
    "#     \"AC\": [[0,1,1],\n",
    "#             [1,0,1],\n",
    "#             [1,1,0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can either generate a graph randomly, or select from a small subset for a pre-defined Markov equivalence class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n,d,p,sigma):\n",
    "    \"\"\"\n",
    "        n: number of samples per node\n",
    "        k: number of nodes\n",
    "        p: prob. of edge\n",
    "        sigma: noise term for gaussian\n",
    "    \"\"\"\n",
    "    draw_ints = ((-2,-0.5),(0.5,2))\n",
    "    # graph of d nodes, n samples each\n",
    "    X = np.zeros((n,d))\n",
    "    B = X.copy()\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,d):\n",
    "            if np.random.choice((0,1),size=1,p=((1-p),p)):\n",
    "                I1 = np.random.uniform(draw_ints[0][0],draw_ints[0][1],1)[0]\n",
    "                I2 = np.random.uniform(draw_ints[1][0],draw_ints[1][1],1)[0]\n",
    "                X[i,j] = np.random.choice((I1,I2),size=1,p=[.5,.5]) + np.random.normal(loc=0,scale=sigma,size=1)\n",
    "                B[i,j] = 1\n",
    "    y = X.T\n",
    "    return y, X, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dataset(N,true_g,sigma):\n",
    "    \n",
    "    if true_g == \"A1\":\n",
    "        x1 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x2 = x1 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = x2 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        \n",
    "    elif true_g == \"A2\":\n",
    "        x2 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x1 = x2 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = x1 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "      \n",
    "    elif true_g == \"A3\":\n",
    "        x2 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = np.random.normal(loc=0,scale=sigma,size=N)        \n",
    "        x1 = x2 + x3 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "    \n",
    "    elif true_g == \"A4\":\n",
    "        x1 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x2 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        \n",
    "    elif true_g == \"A5\":\n",
    "        x1 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x2 = x1 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = x1 + x2 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "\n",
    "    elif true_g == \"A6\":\n",
    "        x1 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x2 = np.random.normal(loc=0,scale=sigma,size=N)\n",
    "        x3 = x1 + x2 + np.random.normal(loc=0,scale=sigma,size=N)\n",
    "    else:\n",
    "        print(f\"Graph {true_g} not found.\")\n",
    "        \n",
    "    x = np.c_[x1,x2,x3]\n",
    "    y = x.T\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(beta, y, x):\n",
    "\n",
    "    mu = beta.dot(x.T)\n",
    "    neg_llik = -1*norm(loc=mu, scale=1).logpdf(y).sum()\n",
    "\n",
    "    return neg_llik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Gym-like wrapper\n",
    "class LabeledMDP():\n",
    "    def __init__(self, true_graph, generate_data=True, max_time = 200):\n",
    "        if generate_data:\n",
    "            self.true_graph=true_graph\n",
    "            self.n_nodes = self.true_graph.shape[1]\n",
    "        \n",
    "        else: # grab from adjacency\n",
    "            self.true_graph = np.asarray(adj_matrices[true_graph])\n",
    "            self.true_graph_name = true_graph\n",
    "            self.n_nodes = np.asarray(self.true_graph).shape[1] #TODO: should not be shape 0 but 1\n",
    "\n",
    "        self.max_time = max_time\n",
    "        self.generate_data=generate_data\n",
    "\n",
    "        # state space\n",
    "        self.curr_graph = self.get_reset_graph()\n",
    "        self.t = 0 # keep track of the timestep\n",
    "        self.n_experiments = 0 # experiments run so far\n",
    "        self.intervened_nodes = {} # keep track of which nodes have been intervened on\n",
    "        self.obs_dim = np.prod(self.curr_graph.shape) + 2\n",
    "\n",
    "        # action space\n",
    "        # for each node, there are experiments with three different choices n: {10, 100, 1000}\n",
    "        self.experiment_sizes = [5, 20]\n",
    "        self.experiment_actions = np.array(list(itertools.product(np.arange(self.n_nodes), self.experiment_sizes)))\n",
    "        self.n_actions = len(self.experiment_actions) # add one for the stop action\n",
    "\n",
    "    def get_reset_graph(self):\n",
    "        return np.ones((self.n_nodes, self.n_nodes)) # the totally connected graph\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.curr_graph = self.get_reset_graph()\n",
    "        self.intervened_nodes = {}\n",
    "        self.n_experiments = 0\n",
    "        return self.create_state()\n",
    "\n",
    "    def create_state(self):\n",
    "        frac = len(self.intervened_nodes.keys()) / self.n_nodes\n",
    "        s = np.concatenate((np.array(self.curr_graph).flatten(), [self.n_experiments, frac]))\n",
    "        assert s.ndim == 1, \"Not 1D feature vector\"\n",
    "        return s\n",
    "    \n",
    "    def correct_edges_frac(self):\n",
    "        # compare correct and incorrect edges\n",
    "        # use for shaped reward if desired\n",
    "        n_correct_edges = np.sum(self.curr_graph == self.true_graph)\n",
    "        # print(n_correct_edges, self.curr_graph == self.true_graph)\n",
    "        return n_correct_edges / np.prod(self.curr_graph.shape)\n",
    "\n",
    "    def transition_graph(self, node, n_samples):\n",
    "        # define candidate graphs the agent considers\n",
    "        # for now, just set it to be all possible graphs\n",
    "        candidate_graphs = list(adj_matrices.keys())\n",
    "\n",
    "        # std dev of generated data\n",
    "        sigma = 1.\n",
    "        if self.generate_data: # fixed: p = 0.1\n",
    "            x,Y = generate_dataset(n_samples,self.n_nodes,0.1,sigma)\n",
    "        else:\n",
    "            x,Y = select_dataset(n_samples,self.true_graph_name,sigma)\n",
    "        ll=[]\n",
    "        bic_scores = []\n",
    "        for g_name in candidate_graphs:\n",
    "            G = np.array(adj_matrices[g_name])\n",
    "            graph_ll=0\n",
    "            for var in range(self.n_nodes):\n",
    "                data = Y[var]\n",
    "                predictors = np.einsum('ij,ij->ij',np.tile(G[var],(n_samples,1)),x)\n",
    "                opt_res = optimize.minimize(fun = log_lik, \n",
    "                                        x0 = [0, 0, 0], \n",
    "                                        args = (data, predictors))\n",
    "\n",
    "                if opt_res['success'] == False:\n",
    "                    #print(f\"Failed fit for model: {g_name} (true:{true_graph})\")\n",
    "                    graph_ll += -1e10\n",
    "                           \n",
    "                graph_ll += -1*opt_res['fun']\n",
    "            ll.append(graph_ll)\n",
    "            graph_complexity = G.sum()\n",
    "            \n",
    "            # Note: scipy minimize returns the negative log likelihood value\n",
    "            bic_score = graph_ll - .5 * np.log(n_samples) * graph_complexity\n",
    "            bic_scores.append(bic_score)\n",
    "\n",
    "        # transition to graphs w/ probability proportional to BIC scores\n",
    "        # option 1: softmax\n",
    "        p = softmax(bic_scores)\n",
    "        # print(p)\n",
    "        next_ix = np.random.choice(len(candidate_graphs), p = p)\n",
    "        return np.array(adj_matrices[candidate_graphs[next_ix]])\n",
    "\n",
    "\n",
    "    def step(self, a):\n",
    "        assert a >= 0, \"action out of range\"\n",
    "        assert a < self.n_actions, \"action out of range\"\n",
    "        done = False\n",
    "        r = 0\n",
    "        info = 0 # the unshaped reward\n",
    "        if a == self.n_actions - 1:\n",
    "            # STOP action\n",
    "            done = True\n",
    "            if (self.true_graph == self.curr_graph).all():\n",
    "                r += 1\n",
    "                info = 1\n",
    "        else:\n",
    "            # experiment action\n",
    "            node, n_samples = self.experiment_actions[a]\n",
    "            self.n_experiments += n_samples\n",
    "            self.curr_graph = self.transition_graph(node, n_samples)\n",
    "            # compute loss inside agent to make it \n",
    "            r += self.correct_edges_frac() # shaped reward based on number of correct edges\n",
    "            r -= n_samples / (10 * np.max(self.experiment_sizes))# penalty for size of experiment\n",
    "        \n",
    "        self.t += 1\n",
    "#         if self.t >= self.max_time:\n",
    "#             done = True\n",
    "        return self.create_state(), r, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  1.  0. 20.  0.] 0.9 False\n"
     ]
    }
   ],
   "source": [
    "# sanity checks for MDP defined above\n",
    "labeled_mdp = LabeledMDP(\"A1\",generate_data=False)\n",
    "s = labeled_mdp.reset()\n",
    "print(s)\n",
    "\n",
    "sp, r, done, _ = labeled_mdp.step(1)\n",
    "print(sp, r, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types for declaring the neural networks functions simplier and\n",
    "# more intuitive\n",
    "ActivationFn = Callable[[np.ndarray], np.ndarray]\n",
    "Parameters = Mapping[str, np.ndarray]\n",
    "InitFn = Callable[[np.ndarray], Parameters]\n",
    "ForwardFn = Callable[[Parameters, np.ndarray], np.ndarray]\n",
    "Layer = Tuple[InitFn, ForwardFn]\n",
    "\n",
    "\n",
    "# Simple optimizer that applies the following formula to update the parameters:\n",
    "# param' = param - learning_rate * grad\n",
    "def sgd(params: Sequence[Parameters], \n",
    "        gradients: Sequence[Parameters],\n",
    "        learning_rate: float) -> Sequence[Parameters]:\n",
    "    new_parameters = []\n",
    "    for i in range(len(params)):\n",
    "        zipped_grads = zip(params[i].items(), gradients[i].items())\n",
    "        new_parameters.append({\n",
    "            k: v - dv * learning_rate for (k, v), (_, dv) in zipped_grads})\n",
    "\n",
    "    return new_parameters \n",
    "\n",
    "    # Create a linear layer \n",
    "def linear(in_features: int, \n",
    "           out_features: int, \n",
    "           activation: ActivationFn = lambda x: x) -> Layer:\n",
    "\n",
    "    def init_fn(random_key: jnp.ndarray) -> Parameters:\n",
    "        W_key, b_key = jax.random.split(random_key)\n",
    "\n",
    "        x_init = jax.nn.initializers.xavier_uniform() \n",
    "        norm_init = jax.nn.initializers.normal()\n",
    "\n",
    "        W = x_init(W_key, shape=(out_features, in_features))\n",
    "        bias = norm_init(b_key, shape=())\n",
    "        return dict(W=W, bias=bias)\n",
    "\n",
    "    def forward_fn(params: Parameters, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        W = params['W']\n",
    "        bias = params['bias']\n",
    "        return activation(jnp.dot(W, x) + bias)\n",
    "\n",
    "    return init_fn, forward_fn\n",
    "\n",
    "# Creates a sequential model that simply reduces the input over a sequence\n",
    "# of layers\n",
    "def sequential(*layers: Sequence[Layer]):\n",
    "    init_fns, forward_fns = zip(*layers)\n",
    "\n",
    "    def init_fn(random_key: np.ndarray):\n",
    "        layer_keys = jax.random.split(random_key, num=len(layers))\n",
    "        return [init_fn(k) for init_fn, k in zip(init_fns, layer_keys)]\n",
    "\n",
    "    def forward_fn(params: Sequence[Parameters], x: np.ndarray) -> np.ndarray:\n",
    "        for fn, p in zip(forward_fns, params):\n",
    "            x = fn(p, x)\n",
    "        return x\n",
    "\n",
    "    return init_fn, forward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base agent\n",
    "class BaseAgent():\n",
    "    def __init__(self, env_params, agent_params, names_to_suppress=[]):\n",
    "        self.env_params = env_params\n",
    "        self.agent_params = agent_params \n",
    "        self.names_to_suppress = names_to_suppress\n",
    "        self.name = self.generate_name()\n",
    "        self.train_set_list=[]\n",
    "        self.test_set_list=[]\n",
    "\n",
    "        # RL stuff\n",
    "        self.gamma = self.agent_params[\"gamma\"]\n",
    "        self.Q = np.zeros([env_params[\"n_actions\"], env_params[\"obs_dim\"]])\n",
    "\n",
    "        if \"eps\" in self.agent_params:\n",
    "            self.eps = self.agent_params[\"eps\"]\n",
    "\n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def generate_name(self):\n",
    "        # generate agent full name from agent_params and base_name \n",
    "        params = [\"{}={}\".format(name, self.agent_params[name]) for name in self.agent_params if name not in self.names_to_suppress]\n",
    "        return \"{}_{}\".format(self.base_name, '_'.join(params))\n",
    "\n",
    "    def act(self, s):\n",
    "        pass \n",
    "\n",
    "    def eps_greedy_act(self, s):\n",
    "        if self.eval is False:\n",
    "            eps = self.eps\n",
    "        else:\n",
    "            # act greedily if we're evaluating\n",
    "            eps = 0\n",
    "\n",
    "        if np.random.random() < eps:\n",
    "            action = np.random.randint(self.env_params[\"n_actions\"])\n",
    "        else: \n",
    "            Qs = self.Q @ s\n",
    "            assert np.isnan(Qs).sum() == 0\n",
    "            action = np.random.choice(np.where(Qs == Qs.max())[0])\n",
    "        return action\n",
    "  \n",
    "    # def get_eps(self):\n",
    "    #     eps = self.eps_init * (1. - self.frame / self.eps_zero_by) + (self.eps_final) * self.frame / self.eps_zero_by\n",
    "    #     eps = max(min(eps, self.eps_init), self.eps_final)\n",
    "    #     return eps\n",
    "\n",
    "    def step(self, s, a, r, sp, done):\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "        \n",
    "# linear Q-learning\n",
    "class QL(BaseAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.base_name = \"QL\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def act(self, s):\n",
    "        return self.eps_greedy_act(s)\n",
    "\n",
    "    def step(self, s, a, r, sp, done):\n",
    "        self.Q[a, :] += self.agent_params[\"lr\"] * (r + self.agent_params[\"gamma\"] * (1. - done) * np.max(self.Q @ sp) - (self.Q @ s)[a]) * s\n",
    "\n",
    "    def run(self,env,eval=False):\n",
    "        # for stats \n",
    "        self.returns = []\n",
    "        self.unshaped_returns = []\n",
    "        self.returns_and_frame_count = []\n",
    "        self.frame = 0\n",
    "\n",
    "        self.eval = eval\n",
    "\n",
    "        # run it\n",
    "        for episode in range(self.env_params[\"episodes\"]):\n",
    "            s = env.reset()\n",
    "            G = 0\n",
    "            unshaped_G = 0\n",
    "            done = False\n",
    "            self.curr_frame_count = 0\n",
    "            while done is not True:\n",
    "                a = self.act(s)                \n",
    "                sp, r, done, unshaped_r = env.step(a)\n",
    "                \n",
    "                G += (self.gamma ** self.curr_frame_count) * r # if we want to use the discounted return as the eval metric\n",
    "                unshaped_G += (self.gamma ** self.curr_frame_count) * unshaped_r\n",
    "                # G += r # if we want to use total reward as the eval metric\n",
    "\n",
    "                # update agent if we're not evaluating\n",
    "                if eval is False:\n",
    "                    self.step(s, a, r, sp, done)\n",
    "\n",
    "                self.curr_frame_count += 1\n",
    "                if self.curr_frame_count >= self.env_params[\"max_frames_per_ep\"]:\n",
    "                    # NOTE: THIS NEEDS TO BE AFTER self.step() SO THAT WE BOOTSTRAP CORRECTLY\n",
    "                    done = True\n",
    "\n",
    "                # update state\n",
    "                s = sp\n",
    "\n",
    "                self.frame += 1    \n",
    "\n",
    "            self.returns.append(unshaped_G) \n",
    "            self.returns_and_frame_count.append([unshaped_G, self.curr_frame_count])\n",
    "            \n",
    "            # print(\"ep = {} | frame = {} | G = {} | ep length = {}\".format(episode, self.frame - 1, unshaped_G, self.curr_frame_count))\n",
    "\n",
    "# Transition class, could be useful   \n",
    "# A transition or a experience is a \"recorded\" interaction of the agent\n",
    "# with the environment. A Transition contains:\n",
    "#  - state: An observation of the environment\n",
    "#  - next_state: The environment observation after taking an action\n",
    "#  - action: The decision took by the agent at the transition timestep\n",
    "#  - reward: The reported benefit of taking the specified action\n",
    "#  - is_terminal: Whether or not the observation is a \"game over\"\n",
    "class Transition(NamedTuple):\n",
    "    state: np.ndarray\n",
    "    next_state: np.ndarray\n",
    "    action: int\n",
    "    reward: int\n",
    "    is_terminal: bool\n",
    "\n",
    "\n",
    "# DQL\n",
    "class DQL(BaseAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.base_name = \"DQL\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def initialize(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        key = jax.random.PRNGKey(0)\n",
    "        \n",
    "        # Create the deep QNetwork\n",
    "        self.dqn_init_fn, self.dqn_forward_fn = sequential(\n",
    "            linear(self.env.obs_dim, 64, jax.nn.relu),\n",
    "            linear(64, 32, jax.nn.relu),\n",
    "            linear(32, self.env.n_actions)) # out size is the predicted graph\n",
    "\n",
    "        self.key, subkey = jax.random.split(key)\n",
    "        self.dqn_parameters = self.dqn_init_fn(subkey)\n",
    "\n",
    "        # Vectorize the model to work with batches\n",
    "        self.dqn_forward_fn = jax.vmap(self.dqn_forward_fn, in_axes=(None, 0))\n",
    "\n",
    "        # We create a copy of parameters to compute the target QValues \n",
    "        self.target_parameters = self.dqn_parameters\n",
    "        \n",
    "        # We compile the function with jit to improve performance\n",
    "        self.backward_fn = jax.jit(jax.grad(self.compute_loss))\n",
    "\n",
    "        # Declare an SGD optimizer\n",
    "        self.optimizer = partial(sgd, learning_rate=self.agent_params[\"lr\"])\n",
    "        self.optimizer = jax.jit(self.optimizer)\n",
    "\n",
    "\n",
    "    #@jax.grad # Differentiate the loss with respect to the model weights\n",
    "    def compute_loss(self,parameters, x, y, actions):\n",
    "        # Mean squared error as loss function\n",
    "        mse = lambda y1, y2: (y1 - y2) ** 2\n",
    "        # Get the q values corresponding to specified actions\n",
    "        q_values = self.dqn_forward_fn(parameters, x)\n",
    "        q_values = q_values[np.arange(x.shape[0]), actions]\n",
    "        return np.mean(mse(y, q_values))\n",
    "        \n",
    "\n",
    "    def act(self,s,key=0):\n",
    "        key, sk = jax.random.split(key)\n",
    "        # We are using an epsilon greedy policy for exploration\n",
    "        # Meaning that at every timestep we flip a biased coin with EPSILON \n",
    "        # probability of being true and 1- EPSILON of being false. In case it is true\n",
    "        # we take a random action\n",
    "        if jax.random.uniform(sk, shape=(1,)) < EPSILON:\n",
    "            # Remember that we have only 2 actions (left, right)\n",
    "            action = jax.random.randint(sk, shape=(1,), minval=0, maxval=2)\n",
    "        else:\n",
    "            s = np.expand_dims(s, axis=0)\n",
    "            q_values = self.dqn_forward_fn(self.dqn_parameters, s)[0]\n",
    "            # Pick the action that maximizes the value\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        return int(action)\n",
    "    \n",
    "             \n",
    "    def run(self,env,eval=False):\n",
    "        self.returns = []\n",
    "        self.unshaped_returns = []\n",
    "        self.returns_and_frame_count = []\n",
    "        self.frame = 0\n",
    "        \n",
    "        def train():\n",
    "            if len(memory) < self.env_params[\"batch_size\"]:\n",
    "                return self.dqn_parameters # No train because we do not have enough experiences\n",
    "\n",
    "            # Experience replay\n",
    "            transitions = random.sample(memory, k=self.env_params[\"batch_size\"])\n",
    "\n",
    "            # Convert transition into tensors. Converting list of tuples to\n",
    "            # tuple of lists\n",
    "            transitions = Transition(*zip(*transitions))\n",
    "            states = np.array(transitions.state)\n",
    "            next_states = np.array(transitions.next_state)\n",
    "            actions = np.array(transitions.action)\n",
    "            rewards = np.array(transitions.reward)\n",
    "            is_terminal = np.array(transitions.is_terminal)\n",
    "\n",
    "            # Compute the next Q values using the target parameters\n",
    "            # We vectorize the model using vmap to work with batches\n",
    "            self.Q = self.dqn_forward_fn(self.target_parameters, next_states)\n",
    "            # Bellman equation\n",
    "            yj = rewards + self.agent_params[\"gamma\"] * np.max(self.Q, axis=-1)\n",
    "            # In case of terminal state we set a 0 reward\n",
    "            yj = np.where(is_terminal, 0, yj)\n",
    "\n",
    "            # Compute the Qvalues corresponding to the sampled transitions\n",
    "            # and backpropagate the mse loss gradients\n",
    "            gradients = self.backward_fn(self.dqn_parameters, states, yj, actions)\n",
    "\n",
    "            # Update the deep q network parameters using our optimizer\n",
    "            return self.optimizer(self.dqn_parameters, gradients)\n",
    "      \n",
    "     \n",
    "        for episode in range(self.env_params[\"episodes\"]):\n",
    "\n",
    "            state = env.reset()\n",
    "            G = 0\n",
    "            unshaped_G = 0\n",
    "            done = False\n",
    "            curr_frame_count = 0    \n",
    "   \n",
    "            if eval:\n",
    "                self.agent_params[\"eps\"] = 0 # No more random actions    \n",
    "\n",
    "            while done is not True:\n",
    "                for timestep in range(self.env_params[\"max_frames_per_ep\"]):\n",
    "\n",
    "                    if done: \n",
    "                        print(f\"Solved!: {done}\")\n",
    "                        break # The pole has felt, we should start a new episode\n",
    "\n",
    "                    # Take an action\n",
    "                    # With jax is mandatory to provide a random key at any random operation\n",
    "                    key, subkey = jax.random.split(self.key)\n",
    "                    action = agent.act(state,subkey)\n",
    "                    next_state, reward, done, unshaped_r = env.step(action)\n",
    "\n",
    "                    G += (agent.agent_params[\"gamma\"] ** curr_frame_count) * reward # if we want to use the discounted return as the eval metric\n",
    "                    unshaped_G += (agent.agent_params[\"gamma\"] ** curr_frame_count) * unshaped_r\n",
    "\n",
    "                    if not eval: # train\n",
    "                        \n",
    "                        # Generate a transition\n",
    "                        t = Transition(state=state, next_state=next_state, \n",
    "                                       reward=reward, is_terminal=done,\n",
    "                                       action=action)\n",
    "\n",
    "                        memory.append(t) # Store the transition \n",
    "\n",
    "                        self.dqn_parameters = train() # Update the agent with experience replay\n",
    "\n",
    "                    curr_frame_count += 1\n",
    "                    if curr_frame_count >= self.env_params[\"max_frames_per_ep\"]:\n",
    "                        # NOTE: THIS NEEDS TO BE AFTER self.step() SO THAT WE BOOTSTRAP CORRECTLY\n",
    "                        #print(f\"Failed to solve after {curr_frame_count} tries.\")\n",
    "                        done = True\n",
    "\n",
    "                    # Every timestep we reduce the probability of taking random actions\n",
    "                    # this is because we want to explotate what we have learn during the \n",
    "                    # previous episodes\n",
    "                    self.agent_params[\"eps\"] -= 2e-5\n",
    "\n",
    "\n",
    "                    self.returns.append(unshaped_G) \n",
    "                    self.returns_and_frame_count.append([unshaped_G, curr_frame_count])  \n",
    "\n",
    "                    state = next_state\n",
    "               # if episode % 20 == 0:\n",
    "                  #  if not eval: \n",
    "                       # print(f'__Train__ Episode[{episode}]: Reward: {reward}, Unshapped reward: {unshaped_G}')\n",
    "                \n",
    "                # At the end of the episode we update the target parameters\n",
    "                self.target_parameters = self.dqn_parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run QL or DQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch = 1 / 100\n"
     ]
    }
   ],
   "source": [
    "MAX_TRAINING_EPOCHS=100\n",
    "MAX_EPISODES = 1 # Number of max episodes to train the agent\n",
    "MAX_EPISODE_STEPS = 100\n",
    "BATCH_SIZE = 32 # Number of instances used to train at every timestep with experience replay\n",
    "GAMMA = .99 # Discount factor for future rewards\n",
    "EPSILON = .3 # Noise to perform epsilon greedy exploration\n",
    "LEARNING_RATE =1e-3\n",
    "training_set = [\"A1\", \"A2\", \"A3\", \"A6\"]\n",
    "test_set = [\"A4\", \"A5\"]\n",
    "train_set_list = []\n",
    "test_set_list = []\n",
    "\n",
    "env = LabeledMDP(\"A1\",generate_data=False)\n",
    "obs_dim = env.obs_dim\n",
    "n_actions = env.n_actions\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "#H = 5\n",
    "#gamma = 1 - 1 / H\n",
    "\n",
    "agent_params = {\"lr\": LEARNING_RATE, \"eps\": EPSILON, \"gamma\": GAMMA}\n",
    "\n",
    "env_params = {\"batch_size\": BATCH_SIZE,\n",
    "              \"episodes\": MAX_EPISODES, \n",
    "              \"max_frames_per_ep\": MAX_EPISODE_STEPS, \n",
    "              \"n_actions\": n_actions, \n",
    "              \"obs_dim\": obs_dim,\n",
    "             \"input\": training_set}\n",
    "\n",
    "\n",
    "# Could use moving average for result reporting\n",
    "#ma = lambda x, w: np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "# agent\n",
    "agent = DQL(env_params, agent_params)\n",
    "\n",
    "# initialize DQL using env parameters (must comment out if using QL)\n",
    "agent.initialize(env)\n",
    "memory = deque(maxlen=int(1e3))\n",
    "\n",
    "# results\n",
    "train_res = []\n",
    "test_res = []\n",
    "avg_test_res = []\n",
    "\n",
    "# train the agent\n",
    "for _ in range(MAX_TRAINING_EPOCHS):\n",
    "    if _ % 10 == 0:\n",
    "        print(f\"training epoch = {_ + 1} / {MAX_TRAINING_EPOCHS}\")\n",
    "\n",
    "    # sample randomly from the training set\n",
    "    train = training_set[np.random.choice(len(training_set))]\n",
    "    train_set_list.append(train)\n",
    "\n",
    "    # train\n",
    "    env = LabeledMDP(train,generate_data=False)\n",
    "    agent.run(env,eval=False)\n",
    "    train_res.append(agent.returns)  \n",
    "\n",
    "    # test the agent\n",
    "    test_returns = []\n",
    "    for test in test_set:\n",
    "        test_set_list.append(test)\n",
    "        env = LabeledMDP(test,generate_data=False)\n",
    "        agent.run(env, eval = True)\n",
    "        test_returns.append(agent.returns)\n",
    "        test_res.append(test_returns)\n",
    "    avg_test_res.append(np.mean(test_returns))\n",
    "    print(f\"Avg train Return = {np.mean(agent.returns)}\")\n",
    "    print(f\"Avg test Return = {np.mean(test_returns)}\")\n",
    "    \n",
    "train_res = np.array(train_res).squeeze()\n",
    "avg_test_res = np.array(avg_test_res).squeeze()\n",
    "test_res = np.array(test_res).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data from the RL experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,) (2500,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-1616e72532cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Unshapped return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Graph'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-168-1616e72532cc>\u001b[0m in \u001b[0;36mplot_results\u001b[0;34m(train_res, test_res, train_set, test_set)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;31m#+ ['test']*n_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0munshaped_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_training_epochs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ test_res.tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;31m#+ test_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_training_epochs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Unshapped return'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munshaped_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Graph'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,) (2500,) "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# def plot_results(train_res,test_res,train_set,test_set):\n",
    "#     n_train = len(train_res)\n",
    "#     _type = ['train']*n_train*5 #+ ['test']*n_test\n",
    "#     unshaped_r = np.cumsum(train_res) / (np.arange(n_training_epochs*5) + 1) #+ test_res.tolist()\n",
    "#     graph = train_set #+ test_set\n",
    "#     df = pd.DataFrame({'Type':_type, 'Epoch': (np.arange(n_training_epochs*5) + 1),'Unshapped return': unshaped_r, 'Graph': graph})\n",
    "#     sns.lineplot(x='Epoch', y='Unshapped return', hue='Graph',data=df)\n",
    "    \n",
    "# plot_results(train_res,test_res,train_set_list,test_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-53d9dc6ba930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# plot the training and test curves, cumulative avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_TRAINING_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_TRAINING_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_TRAINING_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_test_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_TRAINING_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Episode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,) (5,) "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "matplotlib.rcParams.update({'font.size': 30})\n",
    "\n",
    "figsize = (15, 10)\n",
    "\n",
    "# plot the training and test curves, cumulative avg\n",
    "plt.plot(np.arange(MAX_TRAINING_EPOCHS) + 1, np.cumsum(train_res) / (np.arange(MAX_TRAINING_EPOCHS) + 1), label = \"Training\")\n",
    "plt.plot(np.arange(MAX_TRAINING_EPOCHS) + 1, np.cumsum(avg_test_res) / (np.arange(MAX_TRAINING_EPOCHS) + 1), label = \"Test\")\n",
    "plt.xlabel(\"Training Episode\")\n",
    "plt.ylabel(\"Unshaped Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
